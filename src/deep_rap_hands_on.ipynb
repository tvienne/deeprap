{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> DeepRap </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 : Processing job\n",
    "\n",
    "In this second part, we will run a processing job on a distant computer using Sagemaker SDK : https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor instanciation\n",
    "\n",
    "We will start by instanciating a SageMaker processor to define the processing job. There is two possible types of processors in Sagemaker : SKLearnProcessor, a docker image containing Pandas + Sklearn that can be used for almost all tasks and PySparkProcessor that have to be used for Big Data related tasks. Please instanciate the SKLearnProcessor with the following parameters :\n",
    "\n",
    "- **framework_version** : the version of scikit-learn to use. The list of sklearn versions are available here : https://docs.aws.amazon.com/sagemaker/latest/dg/sklearn.html.\n",
    "\n",
    "- **role** : the IAM role assumed by the processor to collect data and execute the task.\n",
    "\n",
    "- **instance_type** : the type of EC2 instance to use. Find an appropriate instance here : https://aws.amazon.com/fr/sagemaker/pricing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.20.0\"\n",
    "role = \"[Your SageMaker-compatible IAM role]\"\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the processing job\n",
    "\n",
    "To execute the processing job, just use method .run() of your Sklear Processor and specify the following parameters : \n",
    "\n",
    "- **code** : the path to the python file to execute. \n",
    "- **inputs** : the list of input S3 data to be copied on the distant EC2 instance. Please modify the \"source\" with the path to the S3 rap data directory.\n",
    "\n",
    "- **outputs** : the list of output data to be sent back to S3 at the end of the processing job. Please \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "#  Define the inputs and outputs\n",
    "\n",
    "code = \"preprocessing.py\"\n",
    "\n",
    "inputs = [\n",
    "        ProcessingInput(source=\"s3://your-bucket/path/to/your/rap/directory\", destination=\"/opt/ml/processing/input\"),\n",
    "    ]\n",
    "\n",
    "outputs = [\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "    ]\n",
    "\n",
    "# execute the processing job\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=code,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 : model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensorflow' from 'sagemaker.tensorflow' (/Users/Thibaud/opt/anaconda3/lib/python3.7/site-packages/sagemaker/tensorflow/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-45897410f369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensorflow' from 'sagemaker.tensorflow' (/Users/Thibaud/opt/anaconda3/lib/python3.7/site-packages/sagemaker/tensorflow/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import Tensorflow\n",
    "\n",
    "code = 'train.py'\n",
    "role = 'SageMakerRole'\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "framework_version = '1.2.1'\n",
    "\n",
    "tf_estimator = Tensorflow(code,\n",
    "                        role='SageMakerRole',\n",
    "                        instance_type=instance_type,\n",
    "                        instance_count=1,\n",
    "                        framework_version=framework_version)\n",
    "\n",
    "# Starts a SageMaker training job and waits until completion.\n",
    "tf_estimator.fit('s3://my_bucket/my_training_data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : live inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploys the model that was generated by fit() to a SageMaker endpoint\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')\n",
    "\n",
    "response = tf_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tears down the SageMaker endpoint and endpoint configuration\n",
    "tf_predictor.delete_endpoint()\n",
    "\n",
    "# Deletes the SageMaker model\n",
    "tf_predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Generator\n",
    "\n",
    "model = Generator()\n",
    "model.load_weights('../models/model-5-epochs-256-neurons.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIDERMAN'T V.I. I shut spading up\n",
      "I'm hearing to me til you can't come see your girl rolve\n",
      "I'm Savioring Bap Bad Mangion\n",
      "It gets my hearthey the ruce, snaeked us\n",
      "\n",
      "If you mobbin' my chansible bitch\n",
      "Work up fly back\n",
      "Better Belocked they better times\n",
      "She ask me this got couch in my far\n",
      "I'm ready to see that minute me better than I stop what's what I gave every size\n",
      "Gotta tor of green vesigue is about the tale\n",
      "Don't even be so much pose out\n",
      "Why in first ya got you down\n",
      "Skill 'em from my califield maybe and I feel like Coloris\n",
      "Fall from L how to ewepeated him with my new applains\n",
      "You know some shit up I'm never calling Dom Balmoir\n",
      "Whippifuls climb it too through runs I get the situation\n",
      "Don't corper a couple of monster\n",
      "Your brew And onem is\n",
      "It only livin' and live in my people and roll in front them being Poppo\n",
      "And that is we don't get with you to thinking it you can't be up\n",
      "\n",
      "\n",
      "Knop the world and she saying same there\n",
      "He has two-consist sweet send mission cakes up the tune\n",
      "Four headin' homes out mur\n"
     ]
    }
   ],
   "source": [
    "SEED = \"SPIDERMAN\"\n",
    "generatedText = model.predict(start_seed=SEED, gen_size=1000)\n",
    "print(generatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
